{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/kingloogie/QTM-347-Machine-Learning-Final-Project/blob/main/Final_Documentation.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "<div align=\"center\">\n",
        "\n",
        "#  Detecting AI-Generated Faces Using ResNet50: A Transfer Learning Approach\n",
        "\n",
        "**Authors:** Emily Ni, Aaron Tse, Alan Yang, Cynthia Zhang\n",
        "</div>\n"
      ],
      "metadata": {
        "id": "icY0Bxcv5Zf-"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##  Abstract\n",
        "\n",
        "In this project, we investigate whether machine learning models can reliably distinguish between real and AI-generated face images using a limited dataset of 1800 images. We apply ResNet50 and conduct three phases of training: freezing the base model, full fine-tuning, and regularized partial fine-tuning. Our final model, which unfreezes the top 30 layers and incorporates dropout, L2 regularization, and early stopping, achieves a validation accuracy of 97.5%, performing competitively with existing models trained on significantly larger datasets. We further employ Vision Transformers on our dataset, attaining a perfect classification accuracy of 100%, as evidenced by the results of our confusion matrix analysis. These results demonstrate the viability of using transfer learning for AI-generated image detection even under constrained data settings."
      ],
      "metadata": {
        "id": "PxnnBh7e4xtd"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Introduction\n",
        "\n",
        "In today's digital age, AI-generated face images have become increasingly realistic and accessible. Synthetic faces are now frequently exploited for malicious purposes, including online scams, fake identities, and the spread of misleading iinformation on social media.\n",
        "\n",
        "This project addresses the growing concerns about AI-generated faces by applying machine learning methods to systematically distinguish real human faces from AI-generated ones. Solving this problem is crucial for maintaining authenticity in applications such as social platforms, hiring processes, and biometric verification systems. By enhancing our ability to detect AI-generated images, we contribute to protecting individuals’ personal, financial, and even emotional security. Furthermore, this study provides an opportunity to compare the performance of different deep learning architectures, specifically ResNet50 and Vision Transformers. This comparative analysis could offer practical insights into building more robust and generalizable detection systems.\n",
        "\n",
        "To solve this problem, we first thought of convolutional neural networks (CNNs), ResNet-50, and Vision Transformers (ViT). While CNNs are well known for their ability to extract local features, we soon realized that they are limited when it comes to modeling complex structures and capturing long-range dependencies. This made them less suitable for our task, which requires not only detecting subtle local details but also understanding the global structure of an image.\n",
        "\n",
        "Then we decided to focus on comparing ResNet-50 and Vision Transformers. ResNet-50, as shown in previous studies like Keswani (2023), effectively captures fine-grained local features, which could help identify semantic differences between real and generated images. On the other hand, ViT is designed to capture global structure and long-range relationships. According to Malviya et al. (2025), ViT-based models have achieved state-of-the-art performance in detecting AI-generated images, especially from powerful generators such as Stable Diffusion and DALL·E 3.\n",
        "\n",
        "In our experiments, both models worked reasonably well. However, Vision Transformers slightly outperformed ResNet-50, likely because they can better capture global context — an important factor in distinguishing AI-generated images that may look realistic locally but unnatural globally.\n",
        "\n",
        "The key components of our approach include dataset preparation, model training, evaluation, and analysis. After running both models separately and comparing their performance, we found that while both have their strengths, ViT showed a marginal advantage. Nevertheless, a limitation of our study is the relatively small sample size, and future work with larger datasets may provide deeper insights."
      ],
      "metadata": {
        "id": "hAUTfkIZ4zGG"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Vision Transformer Setup\n"
      ],
      "metadata": {
        "id": "A5CBELXE41ns"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Dataset Selection and Preparation:**\n",
        "We began by selecting the 130k Real vs. Fake Face dataset from Kaggle, which contains a large collection of labeled images, including both real photographs and fake images generated by artificial intelligence models. Due to the constraints of our computational resources, we selected a balanced subset of approximately 1,800 images—consisting of ~900 real and ~900 fake samples. This subset was chosen to maintain a diverse representation of image types while keeping the training process computationally manageable."
      ],
      "metadata": {
        "id": "sonL7YXtJl2R"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Environment Setup:**\n",
        "The project environment was configured in Google Colab, leveraging its GPU capabilities to accelerate training. We uploaded the cleaned dataset to Google Drive and ensured that the directory structure matched the requirements of Keras data pipelines. Specifically, separate folders were created for each class (“real” and “fake”), which is critical for Keras’ ImageDataGenerator to automatically label the images during data loading. This step streamlined the data ingestion process and helped avoid manual labeling errors."
      ],
      "metadata": {
        "id": "klBaDQXQJntZ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**ViT Model Configuration and Parameters:**\n",
        "For the Vision Transformer model, the parameters we used for the model include:\n",
        "\n",
        "Batch size — the number of samples processed before the model updates its weights.\n",
        "\n",
        "Learning rate — the step size at each iteration to minimize the loss function.\n",
        "\n",
        "Number of epochs — the number of times the entire dataset is passed through the model.\n",
        "\n",
        "Loss function and optimization steps — controlling how the model measures error and adjusts weights.\n",
        "These parameters play an essential role in optimizing model performance and achieving the best possible generalization on unseen data."
      ],
      "metadata": {
        "id": "LL7xOAWSJ0jS"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Dataset Validation:**\n",
        "Before initiating model training, we conducted a validation check on the dataset. This included verifying the folder structure, confirming class balance, and visually inspecting random samples from each category. As illustrated in our presentation slides, sample images from the “real” and “fake” classes were displayed to ensure data integrity and to provide a qualitative sense of the classification challenge.\n",
        "\n"
      ],
      "metadata": {
        "id": "0Wd7_v9eKFIz"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Model Architecture – Vision Transformer (ViT):**\n",
        "The Vision Transformer architecture is adapted from the transformer models originally developed for natural language processing. Instead of analyzing word sequences, ViTs process images by:\n",
        "\n",
        "1. Dividing input images (224 × 224 pixels) into smaller fixed-size patches (e.g., 16 × 16 pixels).\n",
        "\n",
        "2. Flattening these patches and converting them into embeddings—numerical representations the model can interpret.\n",
        "\n",
        "3. Adding positional embeddings to preserve spatial information.\n",
        "\n",
        "4. Passing the embedded patches through a stack of transformer encoder layers that apply multi-head self-attention to learn relationships across all patches.\n",
        "\n",
        "5. Using the output of a special classification token ([CLS] token) to make the final prediction.\n",
        "\n",
        "This architecture allows the model to capture both local and global features, making it highly effective for tasks like fake image detection.\n",
        "\n"
      ],
      "metadata": {
        "id": "v_skoerEKLPe"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**ViT Implementation Pipeline:**\n",
        "\n",
        "1. Splitting the dataset into training and test sets (80/20 split).\n",
        "\n",
        "2. Using ImageDataGenerator with data augmentation (e.g., random flips, rotations, zoom) to increase model robustness and reduce overfitting.\n",
        "\n",
        "3. Converting all images to 224 × 224 pixels and preparing batches compatible with ViT input specifications.\n",
        "\n",
        "4. Loading a pretrained ViT model from Hugging Face, compiling it with an appropriate optimizer and loss function, and preparing it for fine-tuning on our dataset."
      ],
      "metadata": {
        "id": "Un98zBFEKZ0W"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Vision Transformers Results"
      ],
      "metadata": {
        "id": "ROen6RuGlYCG"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "We conducted a thorough evaluation of Vision Transformers (ViT) applied to our dataset, following a two-phase approach: baseline testing and final performance assessment.\n",
        "\n",
        "**Baseline Evaluation:**\n",
        "\n",
        "To establish a baseline, we began with untuned Vision Transformer evaluations across multiple runs. Using a pretrained ViT model, we ran repeated tests to gauge the stability of performance. The key observations were:\n",
        "\n",
        "Multiple Runs: The pretrained model was evaluated several times to assess its consistency.\n",
        "\n",
        "Stability Metric: The average accuracy across runs was calculated as:\n",
        "\n",
        "Average Accuracy: 0.5748 ± 0.0054\n",
        "\n",
        "This initial figure provided a benchmark for comparison once the model was fine-tuned and trained on our specific dataset.\n",
        "\n"
      ],
      "metadata": {
        "id": "qBS5wR6PYAbZ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Initial Training and Testing:**\n",
        "\n",
        "We then carried out initial training using a carefully prepared subset of the dataset. Over a span of 3 epochs, both training and validation accuracy showed a marked improvement:\n",
        "\n",
        "Epoch 1:\n",
        "\n",
        "Train Accuracy: 96.74%\n",
        "\n",
        "Validation Accuracy: 99.43%\n",
        "\n",
        "Epoch 2:\n",
        "\n",
        "Train Accuracy: 98.95%\n",
        "\n",
        "Validation Accuracy: 99.72%\n",
        "\n",
        "Epoch 3:\n",
        "\n",
        "Train Accuracy: 99.79%\n",
        "\n",
        "Validation Accuracy: 100%\n",
        "\n",
        "Our model performed unexpectedly and exceptionally well. The accuracy curve demonstrated rapid convergence, with the validation set reaching perfect accuracy by the third epoch."
      ],
      "metadata": {
        "id": "A6emeZd4YaTm"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Final Result:**\n",
        "\n",
        "Confusion Matrix Analysis -- For the final evaluation, we analyzed the model's performance using a confusion matrix and detailed classification metrics:\n",
        "\n",
        "Overall Accuracy: 100%\n",
        "\n",
        "Macro Average (Precision, Recall, F1): 1.00\n",
        "\n",
        "The confusion matrix illustrates perfect classification with no misclassifications across the test set. Precision, recall, and F1-scores for both classes (real and fake) achieved the maximum possible value of 1.00, reflecting flawless performance.\n"
      ],
      "metadata": {
        "id": "kg6xGoZOY_3R"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ResNet 50 Setup\n",
        "\n",
        "For the Resnet 50 model, we used the same dataset as ViT and also used Python 3 for model building and training.\n",
        "\n",
        "Parameter:\n",
        "\n",
        "-L2 Regularization: 1e-4\n",
        "\n",
        "-Learning rate: 1e-5\n",
        "\n",
        "-Early stopping patience: 3 epochs\n",
        "\n",
        "-Fine-tuning epochs: Maximum of 7 epochs\n",
        "\n",
        "Structure:\n",
        "\n",
        "-Backbone: ResNet-50 with include_top=False\n",
        "\n",
        "-GlobalAveragePooling2D layer\n",
        "\n",
        "-Dropout layer with a rate of 0.6\n",
        "\n",
        "-Dense layer (128 units, ReLU activation)\n",
        "\n",
        "-Dense output layer (1 unit, sigmoid activation)"
      ],
      "metadata": {
        "id": "LVIe9Iu5lLh0"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ResNet 50 Results"
      ],
      "metadata": {
        "id": "0G6cSCuYlDXL"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Main Results\n",
        "\n",
        "We conducted three stages of training using ResNet50, with performance improving at each step:\n",
        "\n",
        "- **Initial Training (Frozen Base)**  \n",
        "  - Training Accuracy: 63.9%\n",
        "  - Validation Accuracy: 64.2%\n",
        "  - The model began learning basic differences between real and fake faces but was limited by the frozen ResNet50 backbone.\n",
        "\n",
        "- **Full Fine-Tuning (All Layers Unfrozen)**  \n",
        "  - Training Accuracy: 99.6%\n",
        "  - Validation Accuracy: 57.1%\n",
        "  - The model overfit to the training data, showing poor generalization.\n",
        "\n",
        "- **Final Fine-Tuning (Top 30 Layers Unfrozen + Regularization)**  \n",
        "  - Training Accuracy: 95.8%  \n",
        "  - Validation Accuracy: 97.5%\n",
        "  - Validation Loss: 0.1086\n",
        "  - Regularized fine-tuning successfully improved generalization and addressed the overfitting problem.\n",
        "\n",
        "<br>\n",
        "\n",
        "#### Supplementary Results\n",
        "\n",
        "**Model Architecture**  \n",
        "- Base model: `ResNet50(weights='imagenet', include_top=False)`  \n",
        "- Classification head: `GlobalAveragePooling → Dropout → Dense(128 ReLU) → Sigmoid`\n",
        "\n",
        "**Training Strategy**  \n",
        "- **Step 1**: Frozen base, trained classification head only  \n",
        "  - Optimizer: `Adam`, Learning rate: `1e-4`, Epochs: 7  \n",
        "- **Step 2**: Unfroze all layers → severe overfitting  \n",
        "  - Optimizer: `Adam`, Learning rate: `1e-5`, Epochs: 7  \n",
        "- **Step 3**: Unfroze top 30 layers  \n",
        "  - Applied `Dropout(0.6)` and `L2 regularization (1e-4)`  \n",
        "  - Optimizer: `Adam`, Learning rate: `1e-5`  \n",
        "  - Early stopping with `patience=3`\n",
        "\n",
        "<br>\n",
        "\n",
        "#### Summarized Findings and Parameter Choice\n",
        "In our experiments using ResNet50 to classify real versus AI-generated face images, we tested three training strategies. The most important result came from the final model, where we partially unfroze the top 30 layers of ResNet50, added dropout and L2 regularization, and applied early stopping. This model achieved a training accuracy of 95.8% and a validation accuracy of 97.5%, along with a low validation loss of 0.1086. These outcomes indicate strong generalization and confirm that this balanced approach was highly effective. In contrast, training the model with a frozen base yielded only around 64% accuracy, and full fine-tuning without regularization caused severe overfitting, with training accuracy reaching 99.6% but validation accuracy dropping to 57.1%.\n",
        "\n",
        "To support these results, we made careful parameter choices at each stage. We started with a frozen ResNet50 base to avoid overfitting early on and used Adam optimizer with a learning rate of 1e-4. During full fine-tuning, we lowered the learning rate to 1e-5 but observed overfitting due to the absence of regularization. In the final model, we corrected this by unfreezing only the top 30 layers and introducing Dropout (0.6) and L2 regularization (1e-4). Early stopping with patience=3 helped prevent further overfitting. All input images were resized, rescaled, and augmented with flips and small rotations, and zooms to increase robustness on our modest dataset (exactly the same data processing methodology as Visual Transformers to faciliate comparison). These parameters enabled our model to perform effectively despite the dataset's limited size."
      ],
      "metadata": {
        "id": "u5d-Wt26mdkc"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Discussion"
      ],
      "metadata": {
        "id": "ZOlwnkrtvEv1"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Our final ResNet50 model achieved a validation accuracy of 97.5%, which is highly competitive given the dataset size and the only 3 stage of preprocessing. This result aligns well with existing literature, where ResNet-based models trained on large datasets such as CelebA or DeepFakeDetection typically report accuracy in the 90–98% range for binary face classification tasks. Despite using a much smaller dataset (~1,800 images) and simpler augmentation techniques, our model achieved performance at the high end of this benchmark, suggesting that our fine-tuning strategy was both effective and efficient."
      ],
      "metadata": {
        "id": "UqwNZICjwSZg"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Conclusion"
      ],
      "metadata": {
        "id": "0jzzdg-ewmKl"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "We were deeply surprised by the strong performance achieved by both our ResNet and Vision Transformer (ViT) models. Given that our dataset is relatively small, we did not expect such robust results. To investigate further, we also trained a simple convolutional neural network from scratch; to our amazement, it too reached a high degree of accuracy. This consistency across architectures led us to suspect that the underlying classification task may be inherently straightforward or that our cleaned dataset contains very clear, easily distinguishable features. In other words, the simplicity of the data itself may have driven much of the model performance."
      ],
      "metadata": {
        "id": "CSXP4E2mwpJQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "In conclusion, we began by identifying and curating a large collection of images, meticulously cleaning and preprocessing them to ensure consistency. We then dove into foundational deep-learning research to understand the inner workings of ResNet and ViT, tuning their hyperparameters to align with our dataset’s characteristics. In conclusion, both models outperformed our initial expectations, and we believe it shows the importance of the modern architectures and the dataset complexity in model success."
      ],
      "metadata": {
        "id": "3TnRtRD75Cbz"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## References\n",
        "1. https://arxiv.org/abs/2503.18812\n",
        "2. https://medium.com/%40hridaykeswani/detection-of-ai-generated-images-using-rich-and-poor-\n",
        "texture-contrast-fc2024e3e716\n",
        "3. https://www.kaggle.com/datasets/shreyanshpatel1/130k-real-vs-fake-face\n",
        "4. Li, Y., Yang, X., Sun, P., Qi, H., & Lyu, S. (2020). Celeb-DF: a Large-Scale challenging dataset for DeepFake\n",
        "forensics. 2022 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR).\n",
        "https://doi.org/10.1109/cvpr42600.2020.00327\n",
        "5. Rossler, A., Cozzolino, D., Verdoliva, L., Riess, C., Thies, J., & Niessner, M. (2019). FaceForensics++:\n",
        "Learning to Detect Manipulated Facial Images. 2021 IEEE/CVF International Conference on Computer\n",
        "Vision (ICCV), 1–11. https://doi.org/10.1109/iccv.2019.00009"
      ],
      "metadata": {
        "id": "daiZeNR85HXJ"
      }
    }
  ]
}